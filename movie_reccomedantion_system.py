# -*- coding: utf-8 -*-
"""movie_reccomedantion_system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18HdCHBnD63lM7uIRJVlvfUzfM5l4_0nv

# **Load Data**
"""

# datasets
! wget https://files.grouplens.org/datasets/movielens/ml-100k.zip

# unzip
! unzip ml-100k.zip -d /content/data/

"""Melakukan ekstraksi file dari format ZIP untuk mengakses dan menggunakan data di dalamnya."""

# Commented out IPython magic to ensure Python compatibility.
# change directory
# %cd data/ml-100k/

# list directory contents
! ls

# readme file
txt_files = '/content/data/ml-100k/README'
txt_content = open(txt_files, 'r').read()
print(txt_content)

"""Berdasarkan deskripsi dari masing-masing file dan setelah melakukan analisis awal terkait model yang akan digunakan, diputuskan bahwa **u.data dan u.item**
 akan menjadi dataset utama untuk pengembangan model rekomendasi.

+ **u.data:** Berisi informasi rating yang diberikan oleh pengguna terhadap film, sehingga fokus utama file ini adalah pada interaksi antara user dan item.

+ **u.item**: Menyediakan data genre dalam bentuk matriks untuk setiap judul film, sehingga dapat digunakan untuk analisis konten dari masing-masing item.

# **Import Library**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import KFold
from sklearn.metrics import mean_absolute_error, mean_squared_error

import warnings
warnings.filterwarnings("ignore")

"""# **EDA**"""

# define dataset variable
users = pd.read_csv('/content/data/ml-100k/u.data', delimiter='\t', header=None, names=['user_id', 'movie_id', 'rating', 'timestamp'])  # data film (ratings)
# Kolom-kolom yang ada di u.item
column_names = ['movie_id', 'movie_title', 'release_date', 'video_release_date',
                'IMDb_URL', 'unknown', 'Action', 'Adventure', 'Animation',
                'Children\'s', 'Comedy', 'Crime', 'Documentary', 'Drama',
                'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery',
                'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']

# Membaca data u.item dengan encoding yang berbeda untuk menangani karakter non-UTF-8
genres = pd.read_csv('/content/data/ml-100k/u.item', delimiter='|', header=None, names=column_names, encoding='ISO-8859-1')

# Mengecek dan menghitung jumlah nilai unik di masing-masing data frame
print('Jumlah data penonton: ', len(users['user_id'].unique()))
print('Jumlah data genre: ', len(genres['movie_id'].unique()))

"""### ðŸ“Œ Inisialisasi Awal Dataset

Pada tahap awal, dilakukan inisialisasi dua variabel utama:

- **`users`**  
  Menyimpan dataset dari file `u.data`, yang berisi informasi interaksi pengguna berupa rating terhadap film.

- **`genres`**  
  Menyimpan dataset dari file `u.item`, yang memuat informasi konten film, khususnya genre dalam bentuk matriks.

#### ðŸ” Hasil Eksplorasi Awal

- Dataset `users` terdiri dari **943 data unik** (tanpa duplikasi pengguna).
- Dataset `genres` mencakup **1.682 entri film** dengan berbagai genre.

"""

users.head()

genres.head()

users.info()

"""### Pemeriksaan Tipe Data pada `users`

Seluruh tipe data pada dataset `users` telah sesuai dengan kebutuhan proyek, sehingga **tidak diperlukan proses konversi tipe data** pada tahap preprocessing.

#### ðŸ“Š Informasi Tambahan
- Jumlah total entri pada dataset `users`: **99.999 baris data**.

"""

genres.info()

"""**Struktur Dataset `genres`**

Dataset `genres` terdiri dari **22 kolom** dan **1.681 baris data**. Kolom-kolom dari indeks **5 hingga 23** merupakan matriks genre, yang merepresentasikan kategori film dalam bentuk biner:

- Nilai **1** menunjukkan bahwa film termasuk dalam genre tersebut.
- Nilai **0** menunjukkan sebaliknya.

Semua kolom genre memiliki tipe data **integer** dan telah sesuai dengan kebutuhan model content-based filtering.

"""

# Cek jumlah duplikat berdasarkan movie_id
print("Data duplikat pada movie_id pada users: ",users.duplicated().sum())
print("Data duplikat pada user_id pada users: ",genres.duplicated().sum())

"""Hasil pemeriksaan menunjukkan bahwa **tidak terdapat data duplikat** pada kedua dataset, baik pada `users` maupun `genres`."""

genre_columns = genres.columns[5:]  # Kolom genre dimulai dari indeks 5
genres_genre_data = genres[genre_columns]

# Hitung jumlah 1 untuk setiap genre
genre_counts = genres_genre_data.sum()

# Buat bar chart untuk visualisasi
plt.figure(figsize=(10, 6))
ax = genre_counts.plot(kind='bar', color='skyblue')

# Menambahkan label jumlah di atas setiap bar
for i, count in enumerate(genre_counts):
    ax.text(i, count + 0.5, str(count), ha='center', va='bottom', fontsize=10)

plt.title('Jumlah Film per Genre')
plt.xlabel('Genre')
plt.ylabel('Jumlah Film')
plt.xticks(rotation=90)
plt.show()

"""Visualisasi berikut diambil dari dataset `genres` dan menggambarkan distribusi jumlah penonton berdasarkan genre film. Grafik ini menunjukkan tingkat ketertarikan pengguna terhadap masing-masing genre.

Adapun lima genre dengan jumlah penonton tertinggi adalah sebagai berikut:
1. **Drama**
2. **Comedy**
3. **Action**
4. **Thriller**
5. **Romance**
"""

plt.figure(figsize=(8, 6))
plt.hist(users['rating'], bins=5, edgecolor='black', color='skyblue')
plt.title('Distribusi Rating Pengguna')
plt.xlabel('Rating')
plt.ylabel('Frekuensi')
plt.show()

"""Rata-rata rating yang diberikan oleh pengguna berada dalam kisaran antara **2,6 hingga 4,3**."""

movie_avg_rating = users.groupby('movie_id')['rating'].mean()
top_avg_rated_movies = movie_avg_rating.head(20)  # 20 film dengan rating tertinggi

top_avg_rated_movies = pd.merge(top_avg_rated_movies, genres[['movie_id', 'movie_title']], left_index=True, right_on='movie_id')

plt.figure(figsize=(10, 6))
plt.bar(top_avg_rated_movies['movie_title'], top_avg_rated_movies['rating'], color='lightblue')
plt.title('20 Film Teratas Berdasarkan Rata-rata Rating')
plt.xlabel('Movie Title')
plt.ylabel('Rata-rata Rating')
plt.xticks(rotation=90)
plt.show()

"""Dari total **1.681 data film** pada dataset `genres`, visualisasi ini menunjukkan **20 film dengan rating tertinggi** berdasarkan rating yang diberikan oleh pengguna. Data ini diambil dari hasil penggabungan dataset `users` dan `genres`, dengan **movie_id** sebagai primary key (PK) yang menghubungkan keduanya."""

# Hitung jumlah film per tahun
top5_years = genres['release_date'].value_counts().nlargest(5)

# Tampilkan
print("Top 5 tahun dengan jumlah film terbanyak:")
print(top5_years)

# Visualisasi
plt.figure(figsize=(10, 5))
top5_years.sort_values().plot(kind='barh', color='skyblue')
plt.title('Top 5 Tahun dengan Jumlah Film Terbanyak')
plt.xlabel('Jumlah Film')
plt.ylabel('Tahun')
plt.grid(axis='x')
plt.tight_layout()
plt.show()

"""Tahun **1995** adalah tahun dengan produksi film terbanyak, yaitu sebanyak **215 film** ðŸŽ¬. Diikuti oleh tahun **1994** dan **1993**, sementara tahun **1997** menempati posisi keempat dengan jumlah produksi film tertinggi ðŸ“…. Pola ini kembali terulang di tahun **1992**, yang menempati posisi kelima dalam daftar produksi film terbanyak ðŸ”.

"""

#remove year
genres['release_date'] = genres.movie_title.str.extract('([0-9]{4})')
genres['movie_title'] = genres['movie_title'].apply(lambda x: x.split('(', 1)[0].strip() if '(' in x else x)

genres.head()

"""Pemisahan tahun dari kolom `movie_title` dilakukan dengan tujuan untuk menggantikan nilai pada kolom `release_date` dengan tahun yang diambil langsung dari judul film. Hal ini bertujuan untuk membuat tampilan data menjadi lebih rapi dan ringkas.

# **Data Preparation**
"""

print("Data genre yang bernilai null: ",genres.isnull().sum())
print("\nData users yang bernilai null: ",users.isnull().sum())

"""Dataset `users` tidak memiliki nilai yang hilang, sehingga seluruh data pada kolom tersebut lengkap.  
Namun, pada dataset `genres`, terdapat **3 kolom** yang mengandung nilai **null**.  
Untuk kolom `video_release_date`, sepertinya semua data memiliki nilai **NaN**. Selain itu, terdapat **1 data null** pada kolom `release_date` dan **3 data null** pada kolom `IMDb_url`.
"""

genres.drop('video_release_date', axis=1, inplace=True)
genres['release_date'].fillna("Unknown", inplace=True)
genres['IMDb_URL'].fillna("Unknown", inplace=True)

"""Data **null** pada kolom `release_date` dan `IMDb_url` diisi dengan nilai **"unknown"**, sementara kolom `video_release_date` dihapus, karena ketiga kolom tersebut tidak mempengaruhi proses rekomendasi yang akan dibangun."""

# Encode user_id
user_encoder = LabelEncoder()
users['user_index'] = user_encoder.fit_transform(users['user_id'])

# Encode movie_id
movie_encoder = LabelEncoder()
users['movie_index'] = movie_encoder.fit_transform(users['movie_id'])

"""Proses **LabelEncoder** dilakukan untuk mengonversi indeks pada dataset `users` dan `genres` agar tidak terlalu besar, sehingga dapat mencegah terjadinya **sparse matrix** saat diproses dengan model **SVD**. Sparse matrix yang besar dapat mempengaruhi performa model **collaborative filtering** secara negatif.  
Untuk itu, dibuatlah dua kolom baru, yaitu **`user_index`** dan **`movie_index`**, yang berfungsi sebagai representasi indeks yang lebih efisien.
"""

user_id_map = dict(zip(users['user_index'], users['user_id']))
movie_id_map = dict(zip(users['movie_index'], users['movie_id']))

"""### Tujuan Mapping
Mapping dilakukan untuk memastikan bahwa **LabelEncoder** tidak mengganggu data asli, yang nantinya akan digunakan dalam model **content-based filtering**. Dengan melakukan mapping, data asli dapat dipasangkan dengan data indexing, sehingga memudahkan untuk mengembalikannya ke bentuk semula ketika diperlukan.

### Proses Mapping
Data asli yang telah diproses dengan **LabelEncoder** kemudian dipasangkan dengan data indexing yang baru. Pasangan data ini kemudian disimpan dalam bentuk **dictionary**, yang akan sangat berguna saat kita perlu mengonversi kembali ke data aslinya di tahap selanjutnya

# **Modelling**

## **Collaborative Filtering**
"""

# Hyperparameter
n_factors = 20
n_epochs = 30
learning_rates = [0.001, 0.005, 0.01, 0.05]
k_folds = 5

# Menyimpan skor evaluasi
mae_scores = []
rmse_scores = []

# Jumlah user dan item
num_users = users['user_index'].nunique()
num_items = users['movie_index'].nunique()

def train_svd(train_data, num_users, num_items, n_factors, n_epochs, learning_rate):
    user_factors = tf.Variable(tf.random.normal([num_users, n_factors], stddev=0.1))
    item_factors = tf.Variable(tf.random.normal([num_items, n_factors], stddev=0.1))
    optimizer = tf.optimizers.Adam(learning_rate)

    for epoch in range(n_epochs):
        with tf.GradientTape() as tape:
            u_idx = train_data['user_index'].values
            i_idx = train_data['movie_index'].values
            ratings = train_data['rating'].values.astype(np.float32)

            u_vecs = tf.gather(user_factors, u_idx)
            i_vecs = tf.gather(item_factors, i_idx)
            preds = tf.reduce_sum(u_vecs * i_vecs, axis=1)
            loss = tf.reduce_mean((ratings - preds) ** 2)

        grads = tape.gradient(loss, [user_factors, item_factors])
        optimizer.apply_gradients(zip(grads, [user_factors, item_factors]))

    return user_factors, item_factors

"""Fungsi `train_svd` ini bertujuan untuk melatih model **Singular Value Decomposition (SVD)** secara tradisional menggunakan **TensorFlow**. Proses pelatihan ini dilakukan dengan menggunakan **Adam optimizer** untuk memperbarui parameter matriks **user** dan **movie**, yang berfungsi untuk memprediksi rating yang belum diberikan oleh pengguna.

"""

def evaluate_model(user_factors, item_factors, data):
    u_idx = data['user_index'].values
    i_idx = data['movie_index'].values
    true_ratings = data['rating'].values.astype(np.float32)

    u_vecs = tf.gather(user_factors, u_idx)
    i_vecs = tf.gather(item_factors, i_idx)
    pred_ratings = tf.reduce_sum(u_vecs * i_vecs, axis=1).numpy()

    mae = mean_absolute_error(true_ratings, pred_ratings)
    rmse = np.sqrt(mean_squared_error(true_ratings, pred_ratings))

    return mae, rmse

"""Fungsi evaluasi dari hasil `train_svd` dilakukan dengan menggunakan **MAE (Mean Absolute Error)** dan **RMSE (Root Mean Squared Error)**, yang dihitung dalam bentuk skor untuk mengukur seberapa baik model memprediksi rating yang belum diberikan.

"""

for lr in [0.001, 0.005, 0.01, 0.05]:
    print(f"\nðŸ” Evaluating learning rate = {lr}")

    mae_train_all, rmse_train_all = [], []
    mae_test_all, rmse_test_all = [], []

    kf = KFold(n_splits=5, shuffle=True, random_state=42)

    for fold, (train_idx, test_idx) in enumerate(kf.split(users)):
        print(f"  Fold {fold + 1}/5 in progress...")

        train_data = users.iloc[train_idx]
        test_data = users.iloc[test_idx]

        # Train model
        user_factors, item_factors = train_svd(train_data, num_users, num_items, n_factors, n_epochs, lr)

        # Evaluate on training data
        mae_train, rmse_train = evaluate_model(user_factors, item_factors, train_data)
        mae_train_all.append(mae_train)
        rmse_train_all.append(rmse_train)

        # Evaluate on testing data
        mae_test, rmse_test = evaluate_model(user_factors, item_factors, test_data)
        mae_test_all.append(mae_test)
        rmse_test_all.append(rmse_test)

        print(f"    Train MAE: {mae_train:.4f}, RMSE: {rmse_train:.4f}")
        print(f"    Test  MAE: {mae_test:.4f}, RMSE: {rmse_test:.4f}")

    # Rata-rata per learning rate
    print(f"\nðŸ“Š [Learning Rate: {lr}]")
    print(f"   ðŸ”¹ Rata-rata Train  â†’ MAE: {np.mean(mae_train_all):.4f}, RMSE: {np.mean(rmse_train_all):.4f}")
    print(f"   ðŸ”¹ Rata-rata Test   â†’ MAE: {np.mean(mae_test_all):.4f}, RMSE: {np.mean(rmse_test_all):.4f}")

"""Proses pelatihan dan pengujian dilakukan dengan menggunakan **K-Fold Cross Validation** untuk membagi data menjadi training dan testing set. Fungsi **SVD** diterapkan pada setiap iterasi, diikuti dengan evaluasi menggunakan **MAE** dan **RMSE** pada kedua set tersebut.

Berdasarkan hasil yang diperoleh, learning rate 0.05 memberikan hasil yang lebih maksimal, dan model menunjukkan kemungkinan kecil mengalami **overfitting** maupun **underfitting**.

### Hasil Evaluasi:
- **Rata-rata Training**:
  - MAE: 0.6010
  - RMSE: 0.7687
- **Rata-rata Testing**:
  - MAE: 0.8538
  - RMSE: 1.1045

Meskipun perbedaan antara hasil training dan testing tidak terlalu jauh, model masih memerlukan **fine-tuning** untuk mencapai skor **MAE** dan **RMSE** di bawah 0.5.
"""

# Buat salinan data asli agar tidak mengubah dataframe utama
predicted_ratings_df = users.copy()

# Inisialisasi list kosong untuk menyimpan hasil prediksi
predicted_rating_list = []

# Loop setiap baris: ambil user_id dan movie_id lalu prediksi rating
for _, row in predicted_ratings_df.iterrows():
    uid = int(row['user_index'])  # Use the user_index instead of user_id
    mid = int(row['movie_index'])  # Use the movie_index instead of movie_id

    # Prediksi rating = dot product dari faktor user dan item
    pred_rating = np.dot(user_factors[uid], item_factors[mid])
    predicted_rating_list.append(pred_rating)

# Tambahkan kolom baru ke dataframe
predicted_ratings_df['predicted_rating'] = predicted_rating_list

# Contoh tampilan data
print(predicted_ratings_df.head())

"""Dari hasil pelatihan **SVD** yang telah dilakukan, dibuatlah kolom baru bernama **predicted_rating** yang menyimpan hasil prediksi rating untuk setiap pasangan **user** dan **movie**. Kolom ini menunjukkan keakuratan prediksi model terhadap rating yang seharusnya diberikan oleh pengguna, dengan memperhitungkan matriks **user** dan **movie** yang telah dilatih.

## **Content-Based Filtering**
"""

user_movie = pd.merge(users, genres, on="movie_id")

"""Untuk keperluan **content-based filtering**, dilakukan penggabungan dataset **users** dan **genres** berdasarkan **movie_id**."""

user_movie

# Pilih satu user_id, misalnya user 42
target_user_id = 42

# Ambil semua film dengan rating >= 4 (disukai user)
liked_movies = user_movie[(user_movie["user_id"] == target_user_id) & (user_movie["rating"] >= 4)]

# Ambil hanya vektor genre film-film tersebut
genre_columns = genres.columns[4:]
liked_genres_matrix = liked_movies[genre_columns]

"""Untuk memudahkan proses perhitungan, dipilih **user dengan ID 42** sebagai target untuk diproses. Rating yang diambil adalah rating yang diberikan oleh pengguna tersebut dengan nilai **lebih besar atau sama dengan 4**.

Dari proses ini, film-film yang memiliki rating di atas 4 disimpan dalam variabel **liked_movies**. Selanjutnya, vektor genre dari film-film tersebut diambil dan disimpan dalam variabel **liked_genres_matrix** untuk digunakan dalam tahap selanjutnya dari **content-based filtering**.

"""

def recommend_similar_from_custom_input(film_name, genre_input, genres_df, genre_cols):
    # Step 1: Format genre input (list lowercase, hapus spasi)
    input_genres = [g.strip().lower() for g in genre_input.split(",")]

    # Step 2: Buat vektor genre 19 dimensi (1 jika cocok, 0 jika tidak)
    genre_vector = []
    for col in genre_cols:
        genre_vector.append(1 if col.lower() in input_genres else 0)
    genre_vector = np.array(genre_vector).reshape(1, -1)

    # Step 3: Hitung cosine similarity dengan semua film di dataset
    dataset_vectors = genres_df[genre_cols].values
    similarities = cosine_similarity(genre_vector, dataset_vectors).flatten()

    # Step 4: Tambahkan similarity ke dataframe dan beri label nama film input
    result = genres_df.copy()
    result["similarity"] = similarities
    result["input_film"] = film_name

    # Step 5: Ambil Top 3 (kecuali jika filmnya memang ada di dataset, maka kita skip itu sendiri)
    top3 = result.sort_values(by="similarity", ascending=False).head(3)

    return top3[["input_film", "movie_title", "similarity"]]

"""Fungsi berikut digunakan untuk menghitung **Cosine Similarity** antara film-film kesukaan dari pengguna target dan film lainnya dalam dataset. Fungsi ini menerima input berupa nama film dari pengguna, yang kemudian diproses untuk memudahkan pencarian dengan mengabaikan perbedaan huruf kapital (uncased).

# **Preferensi**
"""

film_input = "conjuring"
genre_input = "horror, thriller"

top3 = recommend_similar_from_custom_input(film_input, genre_input, genres, genre_columns)
print("ðŸŽ¬ Rekomendasi film mirip dengan input kamu:")
print(top3)

"""Berdasarkan input film yang diberikan, ditemukan 3 film yang memiliki skor **Cosine Similarity** sebesar **1.0**, yang menunjukkan tingkat kesamaan yang sangat tinggi dengan film yang dimasukkan oleh pengguna. Artinya, ketiga film tersebut memiliki genre yang sangat mirip dengan film yang dicari dan dapat menjadi rekomendasi yang sangat relevan.

"""

user_id = 100  # user target

# Buang film yang sudah ditonton
watched_movies = users[users['user_id'] == user_id]['movie_id'].tolist()
watched_movie_indices = [movie_id_map[m] for m in watched_movies]  # label encoded

# Filter the DataFrame 'predicted_ratings_df' instead of the list
all_predictions = predicted_ratings_df[~predicted_ratings_df['movie_index'].isin(watched_movie_indices)] #Filter out watched movies

# Ambil top N rekomendasi
top_n = 5
top_recs = all_predictions.sort_values(by='predicted_rating', ascending=False).head(top_n)

# Kembalikan movie_id dan judul
top_recs['movie_id'] = top_recs['movie_index'].map({v: k for k, v in movie_id_map.items()})
top_recs = top_recs.merge(genres[['movie_id', 'movie_title']], on='movie_id')

print(top_recs[['movie_id', 'movie_title', 'predicted_rating']])

"""Berdasarkan aktivitas pengguna lain, terdapat 5 film yang diprediksi memiliki rating serupa dengan preferensi pengguna target 100. Rating tertinggi yang diberikan untuk film-film tersebut adalah **7.9**, yang menunjukkan bahwa film-film ini kemungkinan besar akan sangat disukai oleh pengguna target 100.

# **Kesimpulan**

Dalam proyek ini, berbagai metode rekomendasi film telah diterapkan, termasuk **Content-Based Filtering** dan **Collaborative Filtering**. Berdasarkan analisis yang dilakukan, dapat disimpulkan bahwa:

1. **Content-Based Filtering** menggunakan **Cosine Similarity** memberikan hasil yang memadai dalam merekomendasikan film berdasarkan genre yang disukai pengguna. Prediksi yang dihasilkan cukup relevan dengan preferensi pengguna.
   
2. **Collaborative Filtering** melalui **SVD** juga berhasil memberikan rekomendasi yang baik, dengan model yang menunjukkan kemungkinan kecil mengalami **overfitting** atau **underfitting**. Namun, masih diperlukan **fine-tuning** untuk mencapai skor MAE dan RMSE yang lebih rendah (di bawah 0.5).

3. Dari hasil evaluasi model, **learning rate 0.05** memberikan hasil yang optimal dalam meningkatkan performa model.

4. Penggunaan **K-Fold Cross Validation** telah membantu memastikan model tidak hanya berkinerja baik pada data pelatihan, tetapi juga pada data pengujian.

5. Rekomendasi film berdasarkan **Cosine Similarity** menunjukkan adanya film yang memiliki tingkat kesamaan yang sangat tinggi dengan film yang disukai pengguna, sehingga dapat menjadi rekomendasi yang sangat relevan.

Secara keseluruhan, meskipun model sudah memberikan hasil yang baik, masih ada ruang untuk pengoptimalan lebih lanjut guna meningkatkan akurasi prediksi dan kualitas rekomendasi.
"""